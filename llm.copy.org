* huggingface
huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Llama-8B

** models saved directory
Linux/macOS: ~/.cache/huggingface/hub
Windows: C:\Users\<YourUsername>\.cache\huggingface\hub
* llamafactory
to install llamafactory-cli must
pip install -e .[torch,metrics]
pip install bitsandbytes>=0.39.0


æ·»åŠ æ–°çš„æ•°æ®é›†åœ¨ ./data/dataset_info.json æŒ‡å®š

CUDA_VISIBLE_DEVICES=5 llamafactory-cli train examples/train_qlora/llama3_lora_sft_otfq.yaml

CUDA_VISIBLE_DEVICES=5 llamafactory-cli train examples/train_qlora/ds_lora_sft_awq.yaml

CUDA_VISIBLE_DEVICES=5 llamafactory-cli train examples/train_lora/DS-R1-Distill-Lama_lora_sft.yaml


CUDA_VISIBLE_DEVICES=0,1,2,3,4 llamafactory-cli train examples/train_qlora/ds_lora_sft_awq.yaml
CUDA_VISIBLE_DEVICES=1,2,3,4 nohup python3 -m vllm.entrypoints.openai.api_server --model /home/ruijie/hjm-tmp/Qwen2-1.5B --gpu_memory_utilization 0.15  --max-model-len 65536 --enforce-eager --tensor-parallel-size 4 --served-model-name Qwen2.5-1.5B --port 8802 > log1.5B.txt 2>&1 &
CUDA_VISIBLE_DEVICES=0,1,2,3 nohup python3 -m vllm.entrypoints.openai.api_server --model /home/ruijie/hjm-tmp/Qwen2-1.5B --gpu_memory_utilization 0.1  --max-model-len 65536 --enforce-eager --tensor-parallel-size 4 --served-model-name Qwen2.5-1.5B --port 8802 > log1.5B.txt 2>&1 &


* ollama
save models in ~/.ollama/models
diff in format with huggingface or other model hubs
Ollama uses a custom format optimized for streaming inference and local serving (usually .tgz or internally converted .modelfile).
Hugging Face typically provides models in formats like safetensors, bin, or gguf (for some optimized ones).
Ollama Meant to be served directly via Ollama CLI or API. Ollama manages loading, optimization, and inference.
Hugging Face models are more raw â€” you manually load them using libraries like transformers or llama.cpp.
Hugging Face models come as pure weights â€” you need to configure loading options yourself (quantization, tokenizer, etc.).
* vllm
vllm serve Qwen/Qwen2.5-1.5B-Instruct --port 18888
download the models in ~/.cache/huggingface/hub
need to pip install --upgrade torch torchvision torchaudio


CUDA_VISIBLE_DEVICES=5 vllm serve ./models/models--suayptalha--DeepSeek-R1-Distill-Llama-3B/snapshots/451b0224786f8e4b538f225e228b261d1c0b271e --port 18888

// nvidia-smi æ˜¾ç¤ºç¡®å®ä½¿ç”¨äº† gpu 5, ä½†æ˜¯ç»“æœè¿˜æ˜¯æŠ¥å‡ºgpu 0 æ˜¾å­˜ä¸å¤Ÿ, æ·»åŠ äº† --tensor-parallel-size 1 ä¹Ÿä¸è¡Œ
5   N/A  N/A   3363458      C   ...e/conda/envs/lfac310/bin/python3.10       7392MiB

* nvidia cuda
nvcc -V
nvidia-smi

3. æŸ¥çœ‹CUDAå¯ç”¨è®¾å¤‡æ•°é‡ï¼ˆPythoné‡Œï¼‰ï¼š
python
Copy
Edit
import torch
print(torch.cuda.device_count())
4. æŸ¥çœ‹å½“å‰è®¾å¤‡åç§°ï¼š
python
Copy
Edit
import torch
print(torch.cuda.get_device_name(0))
ğŸš€ è¿è¡Œç®€å•CUDAæµ‹è¯•
5. æµ‹è¯•CUDAæ˜¯å¦å¯ç”¨ï¼ˆPythonï¼‰ï¼š
python
Copy
Edit
import torch
print(torch.cuda.is_available())
6. æ˜¾ç¤ºå½“å‰è®¾å¤‡å†…å­˜ä¿¡æ¯ï¼š
python
Copy
Edit
import torch
print(torch.cuda.memory_summary())
ğŸ“¥ æ˜¾å­˜ç®¡ç†
7. æ¸…ç†æ˜¾å­˜ï¼š
python
Copy
Edit
import torch
torch.cuda.empty_cache()


ğŸ“¦ å®‰è£…CUDAï¼ˆå¦‚æœä½ ç”¨condaï¼‰
conda search cudatoolkit
conda install cudatoolkit=11.8 -c conda-forge
