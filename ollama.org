source /opt/LLaMA-Factory/llama-env/bin/activate
Show model information
ollama show llama3.2
ollama list: List models on your computer
ollama ps: List which models are currently loaded

ollama stop llama3.2: Stop a model which is currently running

ollama serve: 启动后台服务，之后可以在dify中配置 list 中的模型

* dify
modify .env change port
EXPOSE_NGINX_PORT=6680
NGINX_PORT=6680 // 打开应用时的端口,要一起修改

email: linzhengyuan@ruijie.com.cn
pass: password123
** reset account
 docker exec -it docker-api-1 flask reset-password

 在docker-compose.yaml的api、worker中增加 privileged: true
** OpenBLAS Operation not permitted
 api:
    image: langgenius/dify-api:latest
    restart: always
    privileged: true
    ...

worker:
    image: langgenius/dify-api:latest
    restart: always
    privileged: true
    ...
sandbox:
    image: langgenius/dify-sandbox:0.2.1
    restart: always
    privileged: true
** 添加llama3.1 LLM时需要添加User prompt, 只有System prompt不会生成text
** ollama 配置LLM失败，看log 因为python 库安装失败，需要修改docker-compose.yaml中的mirror
* To allow listening on all local interfaces, you can follow these steps:
If you’re running Ollama directly from the command line, use the
OLLAMA_HOST=0.0.0.0 ollama serve command to specify that it should listen on all local interfaces
Or

Edit the service file: Open /etc/systemd/system/ollama.service and add the following line inside the [Service] section:
Environment="OLLAMA_HOST=0.0.0.0"

Once you’ve made your changes, reload the daemons using the command
sudo systemctl daemon-reload ,
and then restart the service with
sudo systemctl restart ollama.

For a Docker container, add the following to your docker-compose.yml file:

yaml

extra_hosts:
  - "host.docker.internal:host-gateway"
This will allow the Ollama instance to be accessible on any of the host’s networks interfaces. Once your container is running, you can check if it’s accessible from other containers or the host machine using the command:
curl http://host.docker.internal:11434 .

curl http://localhost:11434/api/chat -d '{
  "model": "llama3.1",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'

* where do ollama pull models to
/usr/share/ollama/.ollama if installed by root instead of in ~/.ollama
Storing models in /usr/share/ollama/.ollama allows for centralized management of models and resources. This can simplify updates and maintenance, as changes only need to be made once for all users on the system.

* label
General Explanation of Labels
Labels depend on the type of machine learning task:

1. Supervised Learning:
Labels are the correct answers provided in the training data.
Examples:
Image classification: Label = "cat" for an image of a cat.
Spam detection: Label = "spam" or "not spam" for an email.
Language modeling (your example): Label = the next word/token in the sequence.
2. Unsupervised Learning:
No labels are provided; the model discovers patterns or clusters in the data.
3. Reinforcement Learning:
Labels are indirectly represented as rewards or penalties that guide the model.

Labels act as the "teacher" during training. The model learns to minimize the error between its predictions and the labels by adjusting its internal parameters. This process, called supervised learning, allows the model to generalize and make accurate predictions on new, unseen data.

* Pre-training:
This is the first phase in training the GPT model, where the model learns a general understanding of language by predicting the next word in a sentence (unsupervised learning).
The model is trained on a large corpus of unlabelled text data (e.g., books, websites) to capture grammar, syntax, and semantic relationships.
* Fine-tuning:
After pre-training, the model undergoes a second training phase (supervised learning) using labelled datasets specific to the task of interest.
For example:
A sentiment analysis dataset might label sentences as "positive" or "negative."
Fine-tuning allows the pre-trained model to specialize in this task while leveraging its pre-learned language knowledge.
